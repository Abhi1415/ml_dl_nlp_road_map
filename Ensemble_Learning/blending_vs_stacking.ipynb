{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "blending_vs_stacking.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5-idpJSpfQw",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://www.kdnuggets.com/wp-content/uploads/dataiku-holdout-strategy.jpg)\n",
        "\n",
        "Pros of the hold-out strategy: Fully independent data; only needs to be run once so has lower computational costs.\n",
        "\n",
        "Cons of the hold-out strategy: Performance evaluation is subject to higher variance given the smaller size of the data.\n",
        "\n",
        "![alt text](https://www.kdnuggets.com/wp-content/uploads/dataiku-kfold-strategy.jpg)\n",
        "\n",
        "Pros of the K-fold strategy: Prone to less variation because it uses the entire training set.\n",
        "\n",
        "Cons of the K-fold strategy: Higher computational costs; the model needs to be trained K times at the validation step (plus one more at the test step)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yGqW_nGydFE",
        "colab_type": "text"
      },
      "source": [
        "**stacking**\n",
        "\n",
        "stacking makes prediction by using a meta-model trained from a pool of base models — a pool of base models are first trained using training data and asked to give their prediction; a different meta model is then trained to use outputs from base models to give the final prediction. The process is actually simple. To train a base model, K-fold cross validation technique is used.\n",
        "\n",
        "\n",
        "Step 1: You have Train Data and Test Data. Assume we are using 4-fold cross validation to train base models, the train_data is then divided into 4 parts.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1320/1*yesnizWjGSNGsUmlkhX18w.png)\n",
        "\n",
        "Training data (4-fold) and Testing data\n",
        "\n",
        "Step 2: Using the 4-part train_data, the 1st base model (assuming its a decision tree) is fitted on 3 parts and predictions are made for the 4th part. This is done for each part of the training data. At the end, all instance from training data will have a prediction. This creates a new feature for tain_data, call it pred_m1 (predictions model 1).\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1248/1*yYFpm4Duauymbqmcs7pqTA.png)\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1248/1*zpYK59ERadLpks69gxANAw.png)\n",
        "\n",
        "Model 1 training and prediction using 4-fold cross validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e18ABiAfydGi",
        "colab_type": "text"
      },
      "source": [
        "Step 3: Model 1 (decision tree) is then fitted on the whole training data — no folding is needed this time. The trained model will be used to prediction Test data. So test_data will also have pred_m1.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1248/1*66ItWo6uyEvindU-0S9nnQ.png)\n",
        "\n",
        "Step 4: Step 2 to 3 are repeated for the 2nd model (e.g KNN) and the 3rd model (e.g. SVM). These will give both train_data and test_data two more predictions, pred_m2 and pred_m3.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1248/1*Cuwvlt9nEh70o9RzFSFrTQ.png)\n",
        "\n",
        "Step 5: Now, to train the meta model (assume it’s a logistic regression), we use only the newly added features from the base models, which are [pred_m1, pred_m2, pred_m3]. Fit this meta model on train_data.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1248/1*BhSO1IVsXtbfMN6uIqKydA.png)\n",
        "\n",
        "Step 6: The final prediction for test_data is given by the trained meta model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciNXEbdMpfSp",
        "colab_type": "text"
      },
      "source": [
        "**Blending**\n",
        "\n",
        "is very similar to Stacking. It also uses base models to provide base predictions as new features and a new meta model is trained on the new features and gives the final prediction. The only difference is that training of meta-model is applied on a separate holdout set (e.g 10% of train_data)rather on full and folded training set.\n",
        "\n",
        "Step 1: train_data is split into base_train_data and holdout_set.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1248/1*5RXUF92qwpx-1BkcTejIoQ.png)\n",
        "\n",
        "Step 2: Base models are fitted on base_train_data, and predictions are made on holdout_set and test_data. These will create new prediction features.\n",
        "\n",
        "example pred_m1\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1248/1*pvGUnMAycqvsYAwVe2LpHw.png)\n",
        "\n",
        "\n",
        "\n",
        "Step 3: A new meta-model is then fit on holdout_set with new prediction features. Both original and meta features from holdout_set will be used.\n",
        "\n",
        "\n",
        "Step 4: The trained meta-model is used to make final predictions on the test data using both original and new meta features."
      ]
    }
  ]
}