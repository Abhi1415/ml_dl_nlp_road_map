
Ensemble methods is a machine learning technique that combines several base models in order to produce one optimal predictive model.

The fundamental principle of the ensemble model is that a group of weak learners come together to form a strong learner, which increases the accuracy of the model. 

#Parallel Ensemble Learning(bagging) :  

Algorithms : Random Forest, Bagged Decision Trees, Extra Trees

#Sequential Ensembling Learning(boosting): 

Step 1: The base learner takes all the distributions and assign equal weight or attention to each observation.

Step 2: If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.

Step 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.

Example : Adaboost, Stochastic Gradient Boosting
